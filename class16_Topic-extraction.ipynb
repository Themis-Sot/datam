{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"},"colab":{"name":"class16_Topic-extraction.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-LGkriZFFLfk"},"source":["---\n","\n","# Data Mining: \n","### Exercises - Topic modeling\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"zrbF3re-FLfm"},"source":["We extract topics from unstructured texts"]},{"cell_type":"code","metadata":{"id":"JqcRbUCdFLfn"},"source":["from sklearn.feature_extraction import text\n","from sklearn import datasets, decomposition\n","\n","n_samples = 1000\n","n_features = 1000\n","n_topics = 6\n","n_top_words = 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAyfA5JVFLfo"},"source":["# Loading the dataset\n","The 20 Newsgroups data set\n","\n","The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups\n","The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter: \n","\n"]},{"cell_type":"code","metadata":{"id":"NSyC45w-FLfo"},"source":["dataset = datasets.fetch_20newsgroups(shuffle=True, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"te43jw0bFLfp"},"source":["# From News to Feature Vectors\n","We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n","\n","\n","## Term Frequency-Inverse Document Frequency\n","\n","We want to consider the relative importance of particular words, so we'll use term frequency–inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n","\n","## Mathematical details\n","\n","tf–idf is the product of two statistics, term frequency and inverse document\n","frequency. Various ways for determining the exact values of both statistics\n","exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n","choice is to use the ''raw frequency'' of a term in a document, i.e. the\n","number of times that term ''t'' occurs in document ''d''. If we denote the raw\n","frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n","tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n","include:\n","\n","  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n","  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n","  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: :$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n","\n","The '''inverse document frequency''' is a measure of whether the term is\n","common or rare across all documents. It is obtained by dividing the total\n","number of documents by the number of documents containing the\n","term, and then taking the logarithm of that quotient.\n","\n","$$\\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$$\n","\n","with\n","\n","  * $|D| $: cardinality of D, or the total number of documents in the corpus \n","  * $|\\{d \\in D: t \\in d\\}|$ : number of documents where the term $t$ appears (i.e., $\\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n","\n","Mathematically the base of the log function does not matter and constitutes a\n","constant multiplicative factor towards the overall result.\n","\n","Then tf–idf is calculated as\n","\n","$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"]},{"cell_type":"code","metadata":{"id":"EWEO6MmjFLfp"},"source":["# vectorize the data using the most common words\n","# normalize with TF-IDF weighting (without top 5% stop words)\n","\n","vectorizer = text.CountVectorizer(max_df=0.95,\n","                                  max_features=n_features,\n","                                  stop_words='english')\n","counts = vectorizer.fit_transform(dataset.data[:n_samples])\n","tfidf = text.TfidfTransformer().fit_transform(counts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4f2Z-2LFLfq"},"source":["# Topics extraction"]},{"cell_type":"code","metadata":{"id":"jWSP9d3NFLfq","outputId":"a685aaf7-0090-446f-8e1d-5b64aaa4ab3c"},"source":["# Fit the NMF model\n","print(\"Fitting the NMF model on with n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","nmf = decomposition.NMF(n_components=n_topics)\n","\n","nmf.fit(tfidf)\n","W = nmf.transform(tfidf)\n","H = nmf.components_"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting the NMF model on with n_samples=1000 and n_features=1000...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SWFo46MmFLfr","outputId":"5eb10d40-309c-4837-d3cf-86dbf8de352d"},"source":["H.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6, 1000)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"GHNQG3JWFLfr"},"source":["# Inverse the vectorizer vocabulary to be able\n","feature_names = vectorizer.get_feature_names()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9UrdJGvFLfs"},"source":["Show the top n words in each topic"]},{"cell_type":"code","metadata":{"id":"zwkyOvz-FLfs","outputId":"e8bac906-c9ee-4f4c-cea2-c52573d17ca2"},"source":["for topic_idx, topic in enumerate(H):\n","    print(\"Topic #%d:\" % topic_idx)\n","    print(\" \".join([feature_names[i]\n","                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Topic #0:\n","people god don think like just good time way know say life israel jesus christian bible want did does going\n","()\n","Topic #1:\n","edu university cs host posting article nntp writes cc reply distribution cwru state uiuc game john washington new baseball michael\n","()\n","Topic #2:\n","com hp article writes netcom sun corp stratus posting ca nntp host portal news jim att org distribution systems support\n","()\n","Topic #3:\n","windows uk ac help drive problem thanks use monitor dos software using card file window mail color application pc drivers\n","()\n","Topic #4:\n","clipper key chip encryption government keys public secure use enforcement house law secret brad standard algorithm phone people pat security\n","()\n","Topic #5:\n","nasa gov space jpl center research shuttle moon program laboratory earth distribution henry brian data article sci world long posting\n","()\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N0eXB-JVFLft"},"source":[""],"execution_count":null,"outputs":[]}]}